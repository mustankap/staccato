{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acBn0jSXWmvJ"
   },
   "source": [
    "# Load file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "L4isFoVsWmvN"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\n",
    "DATA_PATH = 'datasets/ravdess_og/'\n",
    "SAMPLE_RATE = 48000\n",
    "\n",
    "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
    "for dirname, _, filenames in os.walk(DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        identifiers = filename.split('.')[0].split('-')\n",
    "        emotion = (int(identifiers[2]))\n",
    "        if emotion == 8: # promeni surprise sa 8 na 0\n",
    "            emotion = 0\n",
    "        if int(identifiers[3]) == 1:\n",
    "            emotion_intensity = 'normal' \n",
    "        else:\n",
    "            emotion_intensity = 'strong'\n",
    "        if int(identifiers[6])%2 == 0:\n",
    "            gender = 'female'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "        \n",
    "        data = data.append({\"Emotion\": emotion,\n",
    "                            \"Emotion intensity\": emotion_intensity,\n",
    "                            \"Gender\": gender,\n",
    "                            \"Path\": file_path\n",
    "                             },\n",
    "                             ignore_index = True\n",
    "                          )\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "4ML1evkjWmvP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files is 1440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion intensity</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>strong</td>\n",
       "      <td>male</td>\n",
       "      <td>datasets/ravdess_og/Actor_15/03-01-02-02-02-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>strong</td>\n",
       "      <td>male</td>\n",
       "      <td>datasets/ravdess_og/Actor_15/03-01-07-02-02-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>strong</td>\n",
       "      <td>male</td>\n",
       "      <td>datasets/ravdess_og/Actor_15/03-01-04-02-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>male</td>\n",
       "      <td>datasets/ravdess_og/Actor_15/03-01-08-01-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>normal</td>\n",
       "      <td>male</td>\n",
       "      <td>datasets/ravdess_og/Actor_15/03-01-05-01-02-02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotion Emotion intensity Gender  \\\n",
       "0       2            strong   male   \n",
       "1       7            strong   male   \n",
       "2       4            strong   male   \n",
       "3       0            normal   male   \n",
       "4       5            normal   male   \n",
       "\n",
       "                                                Path  \n",
       "0  datasets/ravdess_og/Actor_15/03-01-02-02-02-02...  \n",
       "1  datasets/ravdess_og/Actor_15/03-01-07-02-02-02...  \n",
       "2  datasets/ravdess_og/Actor_15/03-01-04-02-02-01...  \n",
       "3  datasets/ravdess_og/Actor_15/03-01-08-01-02-01...  \n",
       "4  datasets/ravdess_og/Actor_15/03-01-05-01-02-02...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"number of files is {}\".format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwyLlFVWmvQ"
   },
   "source": [
    "Number of examples per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E-lc-Gh6WmvQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of examples')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEJCAYAAAB7UTvrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdIklEQVR4nO3de5wcZZ3v8c+XcBUi15GNQJzARnyBHqPMcmTxEsAryFVRsi4Cq0bOAZE9ukdQDqAeBAV012UFw8LiBYEggtwWCSwEXUVIQiCJGAkQNJJNIojhIsEkv/3jebrSmXTPVHq6useZ7/v16tdUPXX7dXXX/Pp5quopRQRmZmYAm3Q7ADMzGz6cFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzAqVJQVJu0m6S9LDkhZI+mQu30HSDEmP5L/b1y1zuqRFkhZKeldVsZmZWWOq6j4FSeOAcRExR9JYYDZwBHA88HREnCfpNGD7iPiMpL2Aq4B9gVcCdwCvjog1lQRoZmYb2LSqFUfEUmBpHn5W0sPALsDhwOQ827eAu4HP5PKrI2IV8LikRaQE8bNm29hpp52it7e3ondgZjYyzZ49+3cR0dNoWmVJoZ6kXuANwM+BnXPCICKWSnpFnm0X4N66xZbksqZ6e3uZNWtW+wM2MxvBJD3RbFrlJ5olbQNcB5waESsHmrVB2QZtW5KmSpoladaKFSvaFaaZmVFxUpC0GSkhXBkRP8jFy/L5htp5h+W5fAmwW93iuwJP9l9nREyLiL6I6OvpaVj7MTOzFlV59ZGAy4CHI+KrdZNuBI7Lw8cBP6wrP0bSFpImABOB+6qKz8zMNlTlOYX9gWOBeZLm5rLPAucB0yV9BPg1cDRARCyQNB34BbAaOMlXHpmZdVaVVx/9hMbnCQAOarLMOcA5VcVkZmYD8x3NZmZWcFIwM7OCk4KZmRWcFMzMrNCRO5qHq97Tbunathefd8iA0x1bY46tNY6tNYPFNhK5pmBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWqCwpSLpc0nJJ8+vKrpE0N78W157dLKlX0h/rpl1SVVxmZtZclV1nXwFcBHy7VhARH6wNS7oQ+EPd/I9GxKQK4zEzs0FUlhQi4h5JvY2mSRLwAeDAqrZvZmYbr1vnFN4CLIuIR+rKJkh6QNJMSW/pUlxmZqNat568NgW4qm58KTA+Ip6StA9wg6S9I2Jl/wUlTQWmAowfP74jwZqZjRYdrylI2hQ4CrimVhYRqyLiqTw8G3gUeHWj5SNiWkT0RURfT09PJ0I2Mxs1utF89HbglxGxpFYgqUfSmDy8OzAReKwLsZmZjWpVXpJ6FfAzYE9JSyR9JE86hvWbjgDeCjwk6UHg+8CJEfF0VbGZmVljVV59NKVJ+fENyq4DrqsqFjMzK8d3NJuZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKVT6j+XJJyyXNrys7W9JvJc3Nr4Prpp0uaZGkhZLeVVVcZmbWXJU1hSuAdzco/1pETMqvWwEk7QUcA+ydl/mGpDEVxmZmZg1UlhQi4h7g6ZKzHw5cHRGrIuJxYBGwb1WxmZlZY904p3CypIdy89L2uWwX4Dd18yzJZWZm1kGdTgoXA3sAk4ClwIW5XA3mjUYrkDRV0ixJs1asWFFJkGZmo1VHk0JELIuINRGxFriUdU1ES4Dd6mbdFXiyyTqmRURfRPT19PRUG7CZ2SgzaFKQtIekLfLwZEmnSNqulY1JGlc3eiRQuzLpRuAYSVtImgBMBO5rZRtmZta6TUvMcx3QJ+kvgctI/8C/Bxw80EKSrgImAztJWgKcBUyWNInUNLQY+DhARCyQNB34BbAaOCki1rTwfszMbAjKJIW1EbFa0pHAP0bEP0t6YLCFImJKg+LLBpj/HOCcEvGYmVlFypxT+JOkKcBxwM25bLPqQjIzs24pkxROAPYDzomIx3Ob/3erDcvMzLph0OajiPiFpM8A4/P448B5VQdmZmadV+bqo0OBucBteXySpBsrjsvMzLqgTPPR2aT7CZ4BiIi5wITKIjIzs64pkxRWR8Qf+pU1vNvYzMz+vJW5JHW+pL8BxkiaCJwC/LTasMzMrBvK1BQ+QerSehVwFbASOLXCmMzMrEvKXH30AvC5/DIzsxGsaVKQdBMDnDuIiMMqicjMzLpmoJrCBR2LwszMhoWmSSEiZtaGJW0OvIZUc1gYES91IDYzM+uwQc8pSDoEuAR4lPQwnAmSPh4R/151cGZm1lllLkm9EDggIhZBer4CcAvgpGBmNsKUuSR1eS0hZI8ByyuKx8zMuqhMTWGBpFuB6aRzCkcD90s6CiAiflBhfGZm1kFlksKWwDLgbXl8BbADcCgpSTgpmJmNEGVuXjuhE4GYmVn3lbn6aAKpq4ve+vkHu3lN0uXAe0nnJF6by84n1TBeIl3NdEJEPCOpF3gYWJgXvzciTtzYN2NmZkNTpvnoBtKzlW8C1m7Euq8ALgK+XVc2Azg9P/P5y8DpwGfytEcjYtJGrN/MzNqsTFJ4MSK+vrErjoh7cg2gvuz2utF7gfdv7HrNzKw6ZZLCP0k6C7id1FMqABExZ4jb/jvgmrrxCZIeIPXCekZE/HiI6zczs41UJim8DjgWOJB1zUeRx1si6XPAauDKXLQUGB8RT0naB7hB0t4RsbLBslOBqQDjx49vNQQzM2ugTFI4Eti9Xf0dSTqOdAL6oIgIgIhYRa6FRMRsSY8CrwZm9V8+IqYB0wD6+vr8BDgzszYqc0fzg8B27diYpHeTTiwflp/TUCvvkTQmD+8OTCTdOW1mZh1UpqawM/BLSfez/jmFwS5JvQqYDOwkaQlwFulqoy2AGZJg3aWnbwW+IGk1sAY4MSKe3vi3Y2ZmQ1EmKZzVyoojYkqD4suazHsdcF0r2zEzs/Ypc0fzzMHmMTOzkWHQcwqS3iTpfknPSXpJ0hpJG1wVZGZmf/7KnGi+CJgCPAJsBXw0l5mZ2QhT5pwCEbFI0piIWAP8m6SfVhyXmZl1QZmk8EJ+RvNcSV8h3Wi2dbVhmZlZN5RpPjo2z3cy8DywG/C+KoMyM7PuKFNT+GNEvAi8CHweQNKelUZlZmZdUaam8GNJH6iNSPoUcH11IZmZWbeUqSlMBqZJOpp0d/PDwL5VBmVmZt0xaE0hIpYCtwH7kZ6+9u2IeK7iuMzMrAvKPI5zBumKo9cCuwKXS7onIj5ddXBmZtZZZc4p/EtEfDginomI+aQawx8qjsvMzLqgTPPRDZLeLOmEXLQ98N1qwzIzs24o0/fRWaRnIJyeizbHScHMbEQq03x0JHAY6cY1IuJJYGyVQZmZWXeUSQov5cdmBoAkd3FhZjZClUkK0yV9E9hO0seAO4BLqw3LzMy6ocxDdi6Q9A5gJbAncGZEzKg8MjMz67gyNQUiYkZE/ENEfLpsQpB0uaTlkubXle0gaYakR/Lf7eumnS5pkaSFkt618W/FzMyGqlRSaNEVwLv7lZ0G3BkRE4E78ziS9gKOAfbOy3xD0pgKYzMzswYqSwoRcQ/wdL/iw4Fv5eFvAUfUlV8dEasi4nFgEe5fycys45omBUl35r9fbuP2ds59KdX6VHpFLt8F+E3dfEtymZmZddBAJ5rHSXobcJikqwHVT4yIOW2MQw3KouGM0lRgKsD48ePbGIKZmQ2UFM4ktfnvCny137QADmxhe8skjYuIpZLGActz+RLSE91qdgWebLSCiJgGTAPo6+trmDjMzKw1TZuPIuL7EfEe4CsRcUC/VysJAeBG4Lg8fBzww7ryYyRtIWkCMBG4r8VtmJlZi8rcp/BFSYcBb81Fd0fEzYMtJ+kq0gN6dpK0BDgLOI90M9xHgF8DR+dtLJA0HfgFsBo4KSLWtPB+zMxsCMo8T+Fc0pVAV+aiT0raPyJOH2AxImJKk0kHNZn/HOCcweIxMxsuek+7pWvbXnzeIZWst8zjOA8BJkXEWgBJ3wIeYF2vqWZmNkKUvU9hu7rhbSuIw8zMhoEyNYVzgQck3UW6dPStuJZgZjYilTnRfJWku4G/IiWFz0TEf1UdmJmZdV6ZmkLt7uMbK47FzMy6rMoO8czM7M+Mk4KZmRUGTAqSNql/HoKZmY1sAyaFfG/Cg5Lc85yZ2ShQ5kTzOGCBpPuA52uFEXFYZVGZmVlXlEkKn688CjMzGxbK3KcwU9KrgIkRcYeklwF+VKaZ2Qg06NVHkj4GfB/4Zi7aBbihwpjMzKxLylySehKwP7ASICIeYd1jNM3MbAQpkxRWRcRLtRFJm9LkUZlmZvbnrUxSmCnps8BWkt4BXAvcVG1YZmbWDWWSwmnACmAe8HHgVuCMKoMyM7PuKHP10dr8YJ2fk5qNFkaEm4/MzEagMo/jPAS4BHiU1HX2BEkfj4h/b2WDkvYErqkr2h04k/Qgn4+RaiUAn42IW1vZhpmZtabMzWsXAgdExCIASXsAtwAtJYWIWAhMyusaA/wWuB44AfhaRFzQynrNzGzoypxTWF5LCNljwPI2bf8g4NGIeKJN6zMzsyFoWlOQdFQeXCDpVmA66ZzC0cD9bdr+McBVdeMnS/owMAv4VET8vk3bMTOzEgaqKRyaX1sCy4C3AZNJbf7bD3XDkjYHDiNd4gpwMbAHqWlpKanZqtFyUyXNkjRrxYoVjWYxM7MWNa0pRMQJFW/7PcCciFiWt7esNkHSpcDNTeKaBkwD6Ovr81VQZmZtVObqownAJ4De+vnb0HX2FOqajiSNy8+CBjgS8MN9zMw6rMzVRzcAl5HuYl7bjo3mnlbfQboZruYrkiaRzlss7jfNzMw6oExSeDEivt7OjUbEC8CO/cqObec2zMxs45VJCv8k6SzgdmBVrTAi5lQWlZmZdUWZpPA64FjgQNY1H0UeNzOzEaRMUjgS2L2++2wzMxuZytzR/CCpXyIzMxvhytQUdgZ+Kel+1j+nMNRLUs3MbJgpkxTOqjwKMzMbFso8T2FmJwIxM7PuK3NH87Oseybz5sBmwPMR8fIqAzMzs84rU1MYWz8u6Qhg36oCMjOz7ilz9dF6IuIGfI+CmdmIVKb56Ki60U2APtY1J5mZ2QhS5uqjQ+uGV5M6qzu8kmjMzKyrypxTqPq5CmZmNkwM9DjOMwdYLiLiixXEY2ZmXTRQTeH5BmVbAx8hdXvtpGBmNsIM9DjO4hnJksYCnwROAK6myfOTzczsz9uA5xQk7QD8H+BDwLeAN0bE7zsRmJmZdd5A5xTOB44CpgGvi4jnOhaVmZl1xUA3r30KeCVwBvCkpJX59ayklUPZqKTFkuZJmitpVi7bQdIMSY/kv9sPZRtmZrbxmiaFiNgkIraKiLER8fK619g29Xt0QERMioi+PH4acGdETATuzONmZtZBG93NRYUOJ523IP89onuhmJmNTt1KCgHcLmm2pKm5bOeIWAqQ/76iS7GZmY1aZbq5qML+EfGkpFcAMyT9suyCOYlMBRg/fnxV8ZmZjUpdqSlExJP573LgelJX3MskjQPIf5c3WXZaRPRFRF9PT0+nQjYzGxU6nhQkbZ1vhkPS1sA7gfnAjcBxebbjgB92OjYzs9GuG81HOwPXS6pt/3sRcZuk+4Hpkj4C/Bo4uguxmZmNah1PChHxGPD6BuVPAQd1Oh4zM1tnOF2SamZmXeakYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmaFjicFSbtJukvSw5IWSPpkLj9b0m8lzc2vgzsdm5nZaNfxZzQDq4FPRcQcSWOB2ZJm5Glfi4gLuhCTmZnRhaQQEUuBpXn4WUkPA7t0Og4zM9tQV88pSOoF3gD8PBedLOkhSZdL2r57kZmZjU5dSwqStgGuA06NiJXAxcAewCRSTeLCJstNlTRL0qwVK1Z0Klwzs1GhK0lB0makhHBlRPwAICKWRcSaiFgLXArs22jZiJgWEX0R0dfT09O5oM3MRoFuXH0k4DLg4Yj4al35uLrZjgTmdzo2M7PRrhtXH+0PHAvMkzQ3l30WmCJpEhDAYuDjXYjNzGxU68bVRz8B1GDSrZ2OxczM1uc7ms3MrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmaFYZcUJL1b0kJJiySd1u14zMxGk2GVFCSNAf4FeA+wFzBF0l7djcrMbPQYVkkB2BdYFBGPRcRLwNXA4V2Oycxs1BhuSWEX4Dd140tymZmZdYAiotsxFCQdDbwrIj6ax48F9o2IT9TNMxWYmkf3BBZ2PNBkJ+B3Xdr2YBxbaxxbaxxba7oZ26sioqfRhE07HckglgC71Y3vCjxZP0NETAOmdTKoRiTNioi+bsfRiGNrjWNrjWNrzXCNbbg1H90PTJQ0QdLmwDHAjV2Oycxs1BhWNYWIWC3pZOBHwBjg8ohY0OWwzMxGjWGVFAAi4lbg1m7HUULXm7AG4Nha49ha49haMyxjG1Ynms3MrLuG2zkFMzPrIieFCki6VdJ2XY6hV9LftLjsc+2Op27dx0u6qKr15230Sppf5TaGi9H0XhuRdIqkhyVd2eU4zpb0aUlfkPT2DmzviKp6e3BSKEFSqXMvSjaJiIMj4pmKwxpML9AwKZR9P2ZDlbuuqdL/Bg6OiA+1uoJ2xhgRZ0bEHe1a3wCOIHUF1HajKilI2lrSLZIelDRf0gclLZa0U57eJ+nuPHy2pGmSbge+nX/h/lDSbbnDvrPyfL35l8o3gDnAbrV1NtpeXmYfSTMlzZb0I0nj6mKsre9SSQsk3S5pK0l75G3PlvRjSa/J818h6f11y9d+5Z8HvEXSXEl/n+O/VtJNwO2StpF0p6Q5kuZJGlJ3IpI+LOmh/F6/I+lQST+X9ICkOyTt3GCZKyRdLOkuSY9Jepuky/P7v2Io8QBjGuzDj0m6P8d4naSX1cVxSd6vv5L03lze7DP/oqRP1r2PcySdMpRgm3w3z8zxzs/fReV598nz/Qw4aSjbbRDHDfk7tkDpRlEkPZff44OS7q19lvk7eW+O8Qu1756kyfkz/R4wr4r9lddzCbA7cKOkz+Xvzv35O3d4nqc3f65z8uuvG8XY4vY/l78Xd5BupF3veJR0nqRf5OPiglw20D67uW7dF0k6vtF68ns4DDhf6fjeo6Ud2ExEjJoX8D7g0rrxbYHFwE55vA+4Ow+fDcwGtsrjxwNLgR2BrYD5ef5eYC3wprr1Libdrdhoe5sBPwV6ctkHSZfe1ubpBVYDk/L4dOBvgTuBibnsfwL/kYevAN5ft/xz+e9k4Oa68uNJNwfukMc3BV6eh3cCFrHuwoPnNnK/7k26s7y2H3cAtq9b30eBC+viuKgu9qsBkfq4Wgm8jvRjZXZtH7TwOTfbhzvWzfP/gU/UxXFb3u7EvJ+2HOQzn5OX3QR4tH7dbfxu7lA3/h3g0Dz8EPC2PHw+ML+Nx0jt+1F7vzsCUbftrwBn5OGbgSl5+MR+373ngQl1n0db91eDY+1LwN/msu2AXwFbAy8DtszlE4FZjWJsYbv7kJLJy4CXk46fT+fv0vtJx8BC1h0D25XYZ/XH60X5+9dsPVdQd9y38zWqagqkD/Htkr4s6S0R8YdB5r8xIv5YNz4jIp7KZT8A3pzLn4iIe0tub0/gtcAMSXOBM0h3btd7PCLm5uHZpIPqr4Fr8zLfBMax8WZExNN5WMCXJD0E3EHqY2qDX/MlHQh8PyJ+B5C3sSvwI0nzgH8gJY5Gbor0LZ8HLIuIeRGxFlhAet+tarQPX5t/Nc4DPtQvpukRsTYiHgEeA16Tyzf4zCNiMfCUpDcA7wQeiIinhhArNP6uHKBU25pH2sd7S9qW9I9hZl7uO0Pcbn+nSHoQuJfUu8BE4CXSPzNYty8B9gOuzcPf67ee+yLicYCK9ld/7wROy8fH3aSkPp70I+zSvA+vZf0mlyLGFrwFuD4iXoiIlWx4k+1K4EXgXyUdBbyQywfaZ400W09lRlXbckT8StI+wMHAuUpNQ6tZ14y2Zb9Fnu+/iibj/ecbaHvXAwsiYr8BQl1VN7yG9M/6mYiY1GDeIv7cvLD5AOutj/NDQA+wT0T8SdJiNnz/ZYkN980/A1+NiBslTSbVvBqpvde1rP++1zK072f/fbgV6dfVERHxYK6aT66bp9ln26z8X0m/5P4CuHwIcaaVNv6unAT0RcRvJJ1N+nwa7eu2yJ/T24H9IuIFpabULYE/5cQNaV+W+Vz6HxNt3V8NCHhfRKzXF1reb8uA15OOkxcHiHFjNf0cIt2Iuy9wEKlnhpNJib2Z+v9DkI/FFtYzZKOqpiDplcALEfFd4ALgjaTq5z55lvcNsop3SNpB0lakEz3/2cL2FgI9kvbL82wmqdmv6JqVwONKHQbWTmi/Pk+rj/9w0i8jgGeBsQOsc1tgeU4IBwCvGiSGgdwJfEDSjjm+HfL6f5unHzeEdbfTWGCppM1ISbHe0ZI2ye2zu7Ouo8Vmn/n1wLuBvyLdgT8kTb4rAL+TtA2pSYJIFzD8QVKtltryCdYGtgV+nxPCa4A3DTL/vaw7Zo4ZZN627q8GfgR8Iv8wItdKIL2npbn2eSypp4R2uAc4Uulc1Vjg0PqJ+TPbNtLNuKcCk/KkZvvsCWAvSVvk2uBBg6xnsOO7ZaOqpkBqrz5f0lrgT8D/Iv2CvEzSZ4GfD7L8T0jV9b8EvhcRsyT1bsz2IuKlfCLq6/nD3xT4R1JzyUA+BFws6QzSP/6rgQeBS4EfSrqP9M+59uvnIWB1bgq4Avh9v/VdCdwkaRYwF/jlINtvKiIWSDoHmClpDfAAqWZwraTfkg6ECa2uv43+H+kzfoLUXFN/UC0EZpJqZSdGxIv5/8sGnzlA/hzvItXg1rQhtkbfzSNynItJ/YLVnABcLukF2vsP9jbgxNykuJD0uQ3kVOC7kj4F3AI0bY6tYH/190XScfRQTgyLgfcC3wCuyz+o7mLotQMAImKOpGtIx84TwI/7zTKWdFzWand/n8tPpcE+y7XB6aTj9hHSMTTQeq4mNYudQjq38Gg73hf4jubScnNDX0Sc3O1YrL2UrnS6OSK+36/8eJp85pI2IV1tdnQ+DzHqKF299ceICEnHkE6gNryKzfsr2Zh91i2jraZgNmRKNw3dTDrROGr/wZGaLS/Kv8yfAf6u0UzeX+sptc+6yTUFMzMrjKoTzWZmNjAnBTMzKzgpmJlZwUnBRj1Ja5T6kKm9TmvDOtfrpVapX62vD3W9ZlXziWYb9SQ9FxHbtHmdk4FPR8R727les6q5pmDWhFJvt1+S9DNJsyS9UalX20clnZjnkaTzlXoynafcEy4b9lJb9IKZ75C+QanXy3sl/Y9cfrZST593K/Uae0oub9jbrlkVfJ+CGWyl1JFazbkRcU0e/k1E7Cfpa6Q7w/cn9UuzALgEOIrU9cDrSb113i/pHuA06moKueZQ83lSp3BHSDoQ+Dbrui94DXAA6U7WhZIuJnUP8WREHJLXtW273rhZf04KZukO00lNptV6v5wHbBMRzwLPSnpR6el6bwauyl03LJM0k9S/z8oBtvdmcv83EfEfknas+0d/S0SsAlZJWk7qdmMecIGkL5PuvO7fpYJZ27j5yGxgg/XiqhbW2WiZ2sm9/r27bhoRv2Jd//3nSjqzhW2aleKkYDY09wAflDRGUg/wVuA+Bu7F8h5y76a5Wel3uU/+htS8B1WztnPzkdmG5xRui4iyl6VeT3pwyoOkX/v/NyL+S9JTrN9L7QN1y5wN/FvujfQFBu9avFEPqmaV8CWpZmZWcPORmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs8J/A8u/o9J4NKjzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=range(8), height=data['Emotion'].value_counts())\n",
    "ax.set_xticks(ticks=range(8))\n",
    "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
    "ax.set_xlabel('Emotions')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIW_k_HBWmvR"
   },
   "source": [
    "number of examples per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AfeMgcr7WmvS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of examples')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Gender'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuTqjg8BWmvS"
   },
   "source": [
    "number of examples per emotion intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bwC_JGI7WmvT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of examples')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Emotion intensity'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYePgCseWmvU"
   },
   "source": [
    "# Load the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_8g9QufkWmvU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed 1439/1440 files"
     ]
    }
   ],
   "source": [
    "mel_spectrograms = []\n",
    "signals = []\n",
    "for i, file_path in enumerate(data.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\n",
    "signals = np.stack(signals,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVW1Av6BWmvV"
   },
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qXduocCHWmvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(1147, 144000), Y_train:(1147,)\n",
      "X_val:(143, 144000), Y_val:(143,)\n",
      "X_test:(150, 144000), Y_test:(150,)\n",
      "Number of unique indexes is 1440, out of 1440\n"
     ]
    }
   ],
   "source": [
    "X = signals\n",
    "train_ind,test_ind,val_ind = [],[],[]\n",
    "X_train,X_val,X_test = [],[],[]\n",
    "Y_train,Y_val,Y_test = [],[],[]\n",
    "for emotion in range(len(EMOTIONS)):\n",
    "    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n",
    "    emotion_ind = np.random.permutation(emotion_ind)\n",
    "    m = len(emotion_ind)\n",
    "    ind_train = emotion_ind[:int(0.8*m)]\n",
    "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
    "    ind_test = emotion_ind[int(0.9*m):]\n",
    "    X_train.append(X[ind_train,:])\n",
    "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
    "    X_val.append(X[ind_val,:])\n",
    "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
    "    X_test.append(X[ind_test,:])\n",
    "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
    "    train_ind.append(ind_train)\n",
    "    test_ind.append(ind_test)\n",
    "    val_ind.append(ind_val)\n",
    "X_train = np.concatenate(X_train,0)\n",
    "X_val = np.concatenate(X_val,0)\n",
    "X_test = np.concatenate(X_test,0)\n",
    "Y_train = np.concatenate(Y_train,0)\n",
    "Y_val = np.concatenate(Y_val,0)\n",
    "Y_test = np.concatenate(Y_test,0)\n",
    "train_ind = np.concatenate(train_ind,0)\n",
    "val_ind = np.concatenate(val_ind,0)\n",
    "test_ind = np.concatenate(test_ind,0)\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
    "# check if all are unique\n",
    "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
    "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQgfbrsXWmvW"
   },
   "source": [
    "# Augment signals by adding AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RbhKUl6kWmvX"
   },
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
    "    signal_len = len(signal)\n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0**(num_bits-1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    # Compute K (covariance matrix) for each noise \n",
    "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
    "    K = np.ones((signal_len, augmented_num)) * K  \n",
    "    # Generate noisy signal\n",
    "    return signal + K.T * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X1toUPgkWmvY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed 1145/1147 files"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/home/ruin/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "aug_signals = []\n",
    "aug_labels = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i,:]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    for j in range(augmented_signals.shape[0]):\n",
    "        aug_labels.append(data.loc[i,\"Emotion\"])\n",
    "        aug_signals.append(augmented_signals[j,:])\n",
    "        data = data.append(data.iloc[i], ignore_index=True)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "aug_signals = np.stack(aug_signals,axis=0)\n",
    "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
    "aug_labels = np.stack(aug_labels,axis=0)\n",
    "Y_train = np.concatenate([Y_train,aug_labels])\n",
    "print('')\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM2L_HIrWmvZ"
   },
   "source": [
    "# Calculate mel spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjOQZFmUWmvZ"
   },
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# test function\n",
    "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
    "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "signal[:len(audio)] = audio\n",
    "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsu9C-Z_Wmva"
   },
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "print(\"Calculatin mel spectrograms for train set\")\n",
    "for i in range(X_train.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_train.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "print('')\n",
    "mel_train = np.stack(mel_train,axis=0)\n",
    "del X_train\n",
    "X_train = mel_train\n",
    "\n",
    "mel_val = []\n",
    "print(\"Calculatin mel spectrograms for val set\")\n",
    "for i in range(X_val.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_val.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
    "print('')\n",
    "mel_val = np.stack(mel_val,axis=0)\n",
    "del X_val\n",
    "X_val = mel_val\n",
    "\n",
    "mel_test = []\n",
    "print(\"Calculatin mel spectrograms for test set\")\n",
    "for i in range(X_test.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_test.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
    "print('')\n",
    "mel_test = np.stack(mel_test,axis=0)\n",
    "del X_test\n",
    "X_test = mel_test\n",
    "\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E71VeUp5Wmvb"
   },
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cb5FUckmWmvc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                       out_channels=16,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 2. conv block\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                       out_channels=32,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 3. conv block\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 4. conv block\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        # LSTM block\n",
    "        self.lstm_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        hidden_size = 128\n",
    "        self.lstm = nn.LSTM(input_size=64,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(0.1)\n",
    "        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(2*hidden_size+256,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        # conv embedding\n",
    "        conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "        # lstm embedding\n",
    "        x_reduced = self.lstm_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(0,2,1) # (b,t,freq)\n",
    "        lstm_embedding, (h,c) = self.lstm(x_reduced) # (b, time, hidden_size*2)\n",
    "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
    "        batch_size,T,_ = lstm_embedding.shape \n",
    "        attention_weights = [None]*T\n",
    "        for t in range(T):\n",
    "            embedding = lstm_embedding[:,t,:]\n",
    "            attention_weights[t] = self.attention_linear(embedding)\n",
    "        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),-1)\n",
    "        attention = torch.bmm(attention_weights_norm,lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n",
    "        attention = torch.squeeze(attention, 1)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([conv_embedding, attention], dim=1) \n",
    "        \n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax, attention_weights_norm\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w51AKkUWmvd"
   },
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10sxtTuNWmvd"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE987u5qWmvd"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits, output_softmax, attention_weights_norm = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP0GGToIWmve"
   },
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax, attention_weights_norm = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbTE4Rr_Wmve"
   },
   "source": [
    "scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqy3qHcoWmve"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = np.expand_dims(X_train,1)\n",
    "X_val = np.expand_dims(X_val,1)\n",
    "X_test = np.expand_dims(X_test,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,c,h,w))\n",
    "\n",
    "b,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,c,h,w))\n",
    "\n",
    "b,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,c,h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDN-lRs7Wmvf"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRbGiqWuWmvf"
   },
   "outputs": [],
   "source": [
    "EPOCHS=1500\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "model = ParallelModel(num_emotions=len(EMOTIONS)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "losses=[]\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # schuffle data\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind,:,:,:]\n",
    "    Y_train = Y_train[ind]\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end-batch_start\n",
    "        X = X_train[batch_start:batch_end,:,:,:]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
    "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwysi2poWmvf"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbJcOvkUWmvg"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_lstm_parallel_model.pt'))\n",
    "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_lstm_parallel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltVwqKGpWmvg"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKXnTrlZWmvg"
   },
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'models')\n",
    "model = ParallelModel(len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_lstm_parallel_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_lstm_parallel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-OAi2f0Wmvg"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXIYDQajWmvh"
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoAen3PtWmvh"
   },
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktWIKtY8Wmvh"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAAfoCzLWmvh"
   },
   "source": [
    "correlation between emotion intensity and corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4qYCNpNWmvi"
   },
   "outputs": [],
   "source": [
    "correct_strong = 0\n",
    "correct_normal = 0\n",
    "wrong_strong = 0\n",
    "wrong_normal = 0\n",
    "for i in range(len(X_test)):\n",
    "    intensity = data.loc[test_ind[i],'Emotion intensity']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  intensity == 'normal':\n",
    "            correct_normal += 1\n",
    "        else:\n",
    "            correct_strong += 1\n",
    "    else: # wrong prediction\n",
    "        if intensity == 'normal':\n",
    "            wrong_normal += 1\n",
    "        else:\n",
    "            wrong_strong += 1\n",
    "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYzjN196Wmvi"
   },
   "source": [
    "correlation between gender and corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7BKkR1WWmvi"
   },
   "outputs": [],
   "source": [
    "correct_male = 0\n",
    "correct_female = 0\n",
    "wrong_male = 0\n",
    "wrong_female = 0\n",
    "for i in range(len(X_test)):\n",
    "    gender = data.loc[test_ind[i],'Gender']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  gender == 'male':\n",
    "            correct_male += 1\n",
    "        else:\n",
    "            correct_female += 1\n",
    "    else: # wrong prediction\n",
    "        if gender == 'male':\n",
    "            wrong_male += 1\n",
    "        else:\n",
    "            wrong_female += 1\n",
    "array = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['male','female'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KfDrZcCWmvi"
   },
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2EuoqsAWmvj"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses,'b')\n",
    "plt.plot(val_losses,'r')\n",
    "plt.legend(['train loss','val loss'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "parallel-cnn-attention-lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
