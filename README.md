# Staccato (final year project)

## Audio system using DRL
This project presents an algorithmic approach to detecting human emotions, by analyzing various vocal and speech patterns. Various algorithms are employed in an ensemble approach to learning directly from speech, music, and other sound signals to create audio-based autonomous systems that have many promising applications in the real world. Acoustically expressed emotions can make communication more efficient be it making a speech, consulting with patients, or talking to robots for that matter. Detecting emotions like anger could provide a clue for the robot to indicate unsafe undesired situations. Several deep neural network-based models have been proposed which establish new state-of-the-art results in effective state evaluation.

[Audio System using DRL](https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0)

[Speech Recog using Librosa](https://github.com/rudrajikadra/Speech-Emotion-Recognition-using-Librosa-library-and-MLPClassifier/blob/master/Speech_Emotion_Recognition_Notebook.ipynb)

[Speech Recog - RAVDESS](https://towardsdatascience.com/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690)

[MFCC](https://towardsdatascience.com/learning-from-audio-the-mel-scale-mel-spectrograms-and-mel-frequency-cepstral-coefficients-f5752b6324a8)

[Librosa - MFCC](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html#librosa.feature.mfcc)

[GRU | LSTM](https://www.kaggle.com/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru)
